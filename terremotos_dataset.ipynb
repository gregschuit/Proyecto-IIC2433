{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Semestral - Entrega final\n",
    "\n",
    "Luciano Davico  \n",
    "Gregory Schuit  \n",
    "\n",
    "En el presente notebook se presenta el análisis de datos sobre terremotos y tsunamis, especificamente mediante la implementación de un Random Forest para poder discriminar entre terremotos que ocasionan tsunamis y los que no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time, sleep\n",
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['time', 'latitude', 'longitude', 'depth', 'mag', 'magType', 'nst',\n",
       "       'gap', 'dmin', 'rms', 'net', 'id', 'updated', 'place', 'type',\n",
       "       'horizontalError', 'depthError', 'magError', 'magNst', 'status',\n",
       "       'locationSource', 'magSource'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leemos los datos de terremotos y filtramos las features.\n",
    "quakes_df = pd.read_csv('Data/quakes.csv')\n",
    "print(len(quakes_df))\n",
    "quakes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.6457</td>\n",
       "      <td>20.8702</td>\n",
       "      <td>10.00</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2018-10-27T18:33:12.420Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65.2234</td>\n",
       "      <td>-151.6636</td>\n",
       "      <td>16.60</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2018-10-27T16:57:27.956Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-22.4704</td>\n",
       "      <td>-68.6516</td>\n",
       "      <td>113.79</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2018-10-27T15:46:43.390Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-56.0652</td>\n",
       "      <td>-27.4276</td>\n",
       "      <td>93.78</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2018-10-27T15:00:46.800Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.3128</td>\n",
       "      <td>20.4930</td>\n",
       "      <td>10.00</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2018-10-27T14:33:26.470Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitude  longitude   depth  mag                      time\n",
       "0   37.6457    20.8702   10.00  4.7  2018-10-27T18:33:12.420Z\n",
       "1   65.2234  -151.6636   16.60  5.3  2018-10-27T16:57:27.956Z\n",
       "2  -22.4704   -68.6516  113.79  4.7  2018-10-27T15:46:43.390Z\n",
       "3  -56.0652   -27.4276   93.78  4.9  2018-10-27T15:00:46.800Z\n",
       "4   37.3128    20.4930   10.00  4.6  2018-10-27T14:33:26.470Z"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quakes_df = quakes_df[['latitude', 'longitude', 'depth', 'mag', 'time']]\n",
    "quakes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1900-07-29T06:59:00.000Z'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(quakes_df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRIMARY_MAGNITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>6.6</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>7.1</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>6.9</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>8.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>1901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PRIMARY_MAGNITUDE  LATITUDE  LONGITUDE  YEAR\n",
       "372                6.6      -4.0      152.0  1900\n",
       "373                7.1      -5.0      148.0  1900\n",
       "374                6.9      -4.0      140.0  1900\n",
       "375                8.4      11.0      -66.0  1900\n",
       "376                6.0      39.0      143.0  1901"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leemos los dato de tsunamis y seleccionamos las features\n",
    "tsunamis_df = pd.read_csv('Data/tsunamis.tsv', sep='\\t', encoding='iso-8859-1')\n",
    "tsunamis_df = tsunamis_df[tsunamis_df['YEAR'] >= 1900]\n",
    "tsunamis_df = tsunamis_df[['PRIMARY_MAGNITUDE', 'LATITUDE', 'LONGITUDE', 'YEAR']]\n",
    "print(len(tsunamis_df))\n",
    "tsunamis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posibe observar que existen 415677 terremotos y tan solo 1145 tsunamis. Este problema de desbalance se nivelará filtrando intervalos con muchos terremotos y pocos tsunamis, es decir, el rango de magnitudes bajas. A continuación observaremos los datos para determinar un punto de corte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Observamos distribuciones de la magnitud\n",
    "hist1 = tsunamis_df['PRIMARY_MAGNITUDE'].hist(bins=40, figsize=(10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist2 = quakes_df['mag'].hist(bins=40, figsize=(10, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de los histogramas, es posible apreciar que la gran mayoria de los terremotos bajo 6 grados aproximadamente, no genera tsunami alguno. Por esta razón es que lo consideraremos como el punto de corte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663\n",
      "4930\n"
     ]
    }
   ],
   "source": [
    "corte = 6.3\n",
    "print(len(tsunamis_df[tsunamis_df['PRIMARY_MAGNITUDE'] > corte]))\n",
    "print(len(quakes_df[quakes_df['mag'] > corte]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsunamis_df = tsunamis_df[tsunamis_df['PRIMARY_MAGNITUDE'] > corte]\n",
    "quakes_df = quakes_df[quakes_df['mag'] > corte]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join\n",
    "Ahora, procederemos a hacer el join de ambas tablas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds left: 2.672\r"
     ]
    }
   ],
   "source": [
    "# Creamos una nueva columna en el dataset de terremotos,\n",
    "# que por defecto será la ausencia de tsunami.\n",
    "quakes_df['tsunami'] = 0\n",
    "\n",
    "t0, i, total = time(), 0, len(quakes_df)\n",
    "for index, row in quakes_df.iterrows():  # Para cada terremoto, se busca la correspondencia\n",
    "    lat = row['latitude']                # en la tabla tsunamis.\n",
    "    lon = row['longitude']\n",
    "    mag = row['mag']\n",
    "    year = int(row['time'][:4])\n",
    "    \n",
    "    delta_pos = 2.5  # valores tuneados a mano para el margen de error.\n",
    "    delta_mag = 0.3\n",
    "    \n",
    "    cond1 = abs(tsunamis_df['LATITUDE'] - lat) < delta_pos\n",
    "    cond2 = abs(tsunamis_df['LONGITUDE'] - lon) < delta_pos\n",
    "    cond3 = abs(tsunamis_df['PRIMARY_MAGNITUDE'] - mag) < delta_mag\n",
    "    cond4 = tsunamis_df['YEAR'] == year\n",
    "    \n",
    "    # Si existe un elemento en la tabla de tsunamis con las features muy cercanas,\n",
    "    # se le asocia al terremoto.\n",
    "    if len(tsunamis_df.loc[cond1 & cond2 & cond3 & cond4]) >= 1:\n",
    "        quakes_df.at[index, 'tsunami'] = 1\n",
    "    else:\n",
    "        quakes_df.at[index, 'tsunami'] = 0\n",
    "    \n",
    "    i += 1\n",
    "    if i % 80 == 0:\n",
    "        print('Seconds left: {:.2f}'.format((time() - t0)/i * (total - i)), end='\\r')\n",
    "\n",
    "print('Total time: {:.2f}'.format(time() - t0))\n",
    "len(quakes_df[quakes_df['tsunami'] == 1]), len(tsunamis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quakes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitudes de todos los terremotos\n",
    "hist3 = quakes_df['mag'].hist(bins=31, figsize=(12,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitudes de terremotos que provocaron tsunamis\n",
    "hist4 = quakes_df[quakes_df['tsunami'] == 1]['mag'].hist(bins=30, figsize=(12,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las siguientes visualizaciones, trataremos de situar los eventos en un scatter plot con respecto a las coordenadas en el globo terrestre, y asi poder tener ideas sobre la distribucion de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = quakes_df['longitude']\n",
    "y = quakes_df['latitude']\n",
    "\n",
    "area = 10\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(x, y, s=area, marker='o', c=1-quakes_df['tsunami'], alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver en el scatter plot, que si pintamos los terremotos que no causan tsunami de amarillo, y los que sí causan tsunami de morado, se trazan las zonas más sísmicas del planeta. Los puntos amarillos logran mostrar, a la izquierda de la visualización, la frontera oeste del continente americano, mientras que a la derecha podemos distinguir la frontera este de Asia y la polinesia. Por conocimiento general, esperabamos esto ya que estas zonas son el límite de la placa tectónica del pacífico, zona donde los volcanes y sísmos abundan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = quakes_df['mag']\n",
    "y = quakes_df['depth']\n",
    "\n",
    "area = 10 # 0 to 10 point radii\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(x, y, s=area, marker='o', c=1-quakes_df['tsunami'], alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score, accuracy_score, \\\n",
    "                            classification_report\n",
    "\n",
    "# Definimos un decorador para tomar el tiempo de las funciones\n",
    "def timer(f_in):\n",
    "    def f_out(*args, **kwargs):\n",
    "        t0 = time()\n",
    "        ret = f_in(*args, **kwargs)\n",
    "        print(\"Total time spent: {:.2f}s\".format(time() - t0), end='\\r')\n",
    "        return ret\n",
    "    return f_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos matriz de features y vector de clase\n",
    "quakes_features = quakes_df[['latitude', 'longitude', 'depth', 'mag']]\n",
    "target = quakes_df['tsunami']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST #\n",
    "@timer\n",
    "def fitRandomForest(X, y, seed=0):\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=75, max_depth=10, \n",
    "                                 class_weight='balanced' , random_state=seed)\n",
    "    trained = clf.fit(X, y)\n",
    "    \n",
    "    return trained\n",
    "\n",
    "\n",
    "# REGRESIÓN LOGÍSTICA #\n",
    "@timer\n",
    "def fitRegresionLogistica(X, y, seed=0):\n",
    "    \n",
    "    clf = LogisticRegression(class_weight='balanced', \n",
    "                             random_state=seed)  # Con este parámetro se ajustan los pesos según la frecuencia de la clase\n",
    "    trained = clf.fit(X, y)\n",
    "    \n",
    "    return trained\n",
    "\n",
    "\n",
    "# SUPPORT VECTOR MACHINE #\n",
    "@timer\n",
    "def fitSupportVectorMachine(X, y, C=1, seed=0):\n",
    "    \n",
    "    clf = SVC(C=C,  # Valor de la penalización \n",
    "              kernel='sigmoid', class_weight='balanced', \n",
    "              probability=True, random_state=seed)\n",
    "    trained = clf.fit(X, y)\n",
    "    \n",
    "    return trained\n",
    "\n",
    "\n",
    "# NEURAL NETWORK\n",
    "@timer\n",
    "def fitNeuralNetwork(X, y, seed=0):\n",
    "    \n",
    "    clf = MLPClassifier(activation='logistic',\n",
    "                        solver='lbfgs',\n",
    "                        early_stopping=True,\n",
    "                        hidden_layer_sizes=(100,),\n",
    "                        random_state=seed)\n",
    "    trained = clf.fit(X, y)\n",
    "    \n",
    "    return trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción \n",
    "def prediction(X_test, trained):\n",
    "    y_pred = trained.predict(X_test)\n",
    "    y_pred_proba = trained.predict_proba(X_test)[:, 1]   # Se indican las prob de predecir la clase 1\n",
    "    return y_pred, y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partición estratificada\n",
    "X_train, X_test, y_train, y_test = train_test_split(quakes_features, target, test_size=0.3,\n",
    "                                                    random_state=1, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> Resultados de métricas evaluadas con Random Forest:\n",
       "</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spent: 0.53s\r",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "no provoca tsunami       0.93      0.93      0.93      1278\n",
      "sí provoca tsunami       0.55      0.58      0.57       201\n",
      "\n",
      "       avg / total       0.88      0.88      0.88      1479\n",
      "\n",
      "Accuracy:  0.8796484110885734 \n",
      "\n",
      "Matriz de Confusión:\n",
      "\n",
      " [[1184   94]\n",
      " [  84  117]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST\n",
    "display(HTML('<h3> Resultados de métricas evaluadas con Random Forest:\\n</h3>'))\n",
    "\n",
    "trained = fitRandomForest(X_train, y_train, seed=1)\n",
    "y_pred, y_pred_proba = prediction(X_test, trained)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['no provoca tsunami', 'sí provoca tsunami']))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred), '\\n')   ## accuracy\n",
    "print('Matriz de Confusión:\\n\\n', confusion_matrix(y_test, y_pred), '\\n')   ## Matriz de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> Resultados de métricas evaluadas con Regresion Logística\n",
       "</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spent: 0.04s\r",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "no provoca tsunami       0.94      0.78      0.85      1278\n",
      "sí provoca tsunami       0.33      0.70      0.45       201\n",
      "\n",
      "       avg / total       0.86      0.77      0.80      1479\n",
      "\n",
      "Accuracy:  0.7660581473968898 \n",
      "\n",
      "Matriz de Confusión:\n",
      "\n",
      " [[993 285]\n",
      " [ 61 140]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# REGRESION LOGÍSTICA\n",
    "display(HTML('<h3> Resultados de métricas evaluadas con Regresion Logística\\n</h3>'))\n",
    "\n",
    "trained = fitRegresionLogistica(X_train, y_train, 1)\n",
    "y_pred, y_pred_proba = prediction(X_test, trained)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['no provoca tsunami', 'sí provoca tsunami']))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred), '\\n')   ## accuracy\n",
    "print('Matriz de Confusión:\\n\\n', confusion_matrix(y_test, y_pred), '\\n')   ## Matriz de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> Resultados de métricas evaluadas con Support Vector Machine\n",
       "</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spent: 1.58s\r",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "no provoca tsunami       0.86      0.51      0.64      1278\n",
      "sí provoca tsunami       0.13      0.46      0.20       201\n",
      "\n",
      "       avg / total       0.76      0.50      0.58      1479\n",
      "\n",
      "Accuracy:  0.49966193373901285 \n",
      "\n",
      "Matriz de Confusión:\n",
      "\n",
      " [[647 631]\n",
      " [109  92]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MAQUINA DE VECTORES DE SOPORTE\n",
    "display(HTML('<h3> Resultados de métricas evaluadas con Support Vector Machine\\n</h3>'))\n",
    "\n",
    "trained = fitSupportVectorMachine(X_train, y_train, C=3, seed=1)\n",
    "y_pred, y_pred_proba = prediction(X_test, trained)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['no provoca tsunami', 'sí provoca tsunami']))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred), '\\n')   ## accuracy\n",
    "print('Matriz de Confusión:\\n\\n', confusion_matrix(y_test, y_pred), '\\n')   ## Matriz de Confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC, es un clasificador que trata de encontrar un hiperplano divisor de los datos. Como podemos intuir de la naturaleza de los datos, estos no son separables por un hiperplano, por lo que es esperable que este clasificador tenga un mal rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> Resultados de métricas evaluadas con Multilayer Perceptron\n",
       "</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time spent: 4.32s\r",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "no provoca tsunami       0.88      0.97      0.92      1278\n",
      "sí provoca tsunami       0.42      0.14      0.21       201\n",
      "\n",
      "       avg / total       0.82      0.86      0.82      1479\n",
      "\n",
      "Accuracy:  0.8573360378634213 \n",
      "\n",
      "Matriz de Confusión:\n",
      "\n",
      " [[1240   38]\n",
      " [ 173   28]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RED NEURONAL\n",
    "display(HTML('<h3> Resultados de métricas evaluadas con Multilayer Perceptron\\n</h3>'))\n",
    "\n",
    "trained = fitNeuralNetwork(X_train, y_train)\n",
    "y_pred, y_pred_proba = prediction(X_test, trained)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['no provoca tsunami', 'sí provoca tsunami']))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred), '\\n')   ## accuracy\n",
    "print('Matriz de Confusión:\\n\\n', confusion_matrix(y_test, y_pred), '\\n')   ## Matriz de Confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del modelo Random Forest queremos aumentar el Recall, para detectar casos en que aparezca la clase minoritaria. Es decir, queremos aumentar la cantidad de casos en que se predice un tsunami, dado que efectivamente ocurrirá.\n",
    "\n",
    "Al incluir el parámetro class_weight='balanced' en la Regresión Logística, las clases adquieren un peso inversamente proporcional a la frecuencia de la clase. Como tenemos una base de datos desbalanceada, los valores 1 de la clase adquieren un peso mayor a los valores 0. Los resultados de esto indican que hay un aumento en el Recall, es decir, se han predicho correctamente un 82,23 % de los Tsunamis que han ocurrido. Esto es un avance, si se considera que con el modelo Random Forest se obtuvo un Recall cercano al 29,3 %. Sin embargo, la Precisión del modelo bajó notablemente a un 16,29 %, por lo que ahora se están prediciendo muchos más tsunamis de los que efectivamente ocurrieron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Subsampling\n",
    "Para poder definir mejor el rendimiento de los modelos, definiremos la función de Random Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SUBSAMPLING #\n",
    "\n",
    "def RandomSubsampling(X_train, y_train, X_test, y_test, B, model=fitRandomForest, seed=None):\n",
    "    \n",
    "    lista_fscore = []\n",
    "    lista_recall = []\n",
    "    lista_precision = []\n",
    "    lista_accuracy = []\n",
    "    \n",
    "    for i in range(B):\n",
    "        trained = model(X_train, y_train, seed)    # Se evalúa modelo de Random Forest\n",
    "        y_pred, y_proba = prediction(X_test, trained)\n",
    "        \n",
    "        recall = recall_score(y_test, y_pred, average='binary')   ## recall\n",
    "        precision = precision_score(y_test, y_pred, average='binary')   ## precision\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)       ## F-score\n",
    "        accuracy = accuracy_score(y_test, y_pred)   ## accuracy\n",
    "        \n",
    "        lista_fscore.append(f_score)\n",
    "        lista_recall.append(recall)\n",
    "        lista_precision.append(precision)\n",
    "        lista_accuracy.append(accuracy)\n",
    "    \n",
    "    return lista_fscore, lista_recall, lista_precision, lista_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para corregir el problema de clases desbalanceada, se utilizará Up-sample Minority Class. Este proceso consiste en aumentar la cantidad de observaciones con la clase minoritaria (en este caso, que ocurra un Tsunami i.e. '1'). Este aumento se da gracias a la duplicación de estos datos de forma aleatoria. Se duplicarán observaciones con reemplazo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Up-sample Minority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4259\n",
       "1     671\n",
       "Name: tsunami, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "quakes_df['tsunami'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_class(X_train, y_train):\n",
    "    train = X_train.join(y_train)\n",
    "\n",
    "    # Se separa la base de datos en dos partes; una solo con eventos de tsunami y otra sin tsunamis.\n",
    "\n",
    "    major = train[train.tsunami == 0] \n",
    "    minor = train[train.tsunami == 1]\n",
    "\n",
    "    # Se duplican los datos de la clase minoritaria en la misma cantidad que en la clase mayoritaria\n",
    "\n",
    "    minor_sampled = resample(minor, replace=True, n_samples=len(major), random_state=1)\n",
    "\n",
    "    # Se crea la nueva base de datos con la base de la clase mayoritaria y la base minoritaria sampleada\n",
    "\n",
    "    quakes_sampled = pd.concat([major, minor_sampled])\n",
    "    print(quakes_sampled['tsunami'].value_counts())\n",
    "\n",
    "    qk_sampled_X = quakes_sampled[['latitude', 'longitude', 'depth', 'mag']]\n",
    "    qk_sampled_y = quakes_sampled['tsunami']\n",
    "    \n",
    "    return qk_sampled_X, qk_sampled_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curva ROC\n",
    "\n",
    "Indica la relación entre falsos positivos y verdaderos positivos en un algoritmo de clasificación binaria. Es una forma más didáctica y gráfica para comparara resultados de distintos modelos de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.lines as mlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qk_sampled_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e08ed3820f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Random Forest con partición estratificada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrained1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitRandomForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqk_sampled_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqk_sampled_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_pred1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_proba1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qk_sampled_X' is not defined"
     ]
    }
   ],
   "source": [
    "# Considerando Up-Sample\n",
    "\n",
    "# Random Forest con partición estratificada\n",
    "trained1 = fitRandomForest(qk_sampled_X, qk_sampled_y, 1)\n",
    "y_pred1, y_pred_proba1 = prediction(X_test, trained1)\n",
    "\n",
    "# Regresión Logística con partición estratificada\n",
    "trained2 = fitRegresionLogistica(qk_sampled_X, qk_sampled_y, 1)\n",
    "y_pred2, y_pred_proba2 = prediction(X_test, trained2)\n",
    "\n",
    "# SVM con partición estratificada\n",
    "trained3 = fitSupportVectorMachine(qk_sampled_X, qk_sampled_y, 1)\n",
    "y_pred3, y_pred_proba3 = prediction(X_test, trained3)\n",
    "\n",
    "# Red Neuronal con partición estratificada\n",
    "trained4 = fitNeuralNetwork(qk_sampled_X, qk_sampled_y, 1)\n",
    "y_pred4, y_pred_proba4 = prediction(X_test, trained4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr: razón de falsos positivos (tasa de predicción de tsunamis, cuando no ocurrieron) -> 1 - Precision\n",
    "# tpr: razón de verdaderos positivos (tasa de predicción de tsunamis, cuando efectivamente hubo) -> Recall\n",
    "# tresholds: puntos en los que serán graficados los valores\n",
    "\n",
    "# y_test obtenido de la partición estratificada e 'y_pred_proba' obtenido de aplicar un Random Forest con Up-Sampled\n",
    "\n",
    "fpr1, tpr1, thresholds1 = roc_curve(np.array(y_test), y_pred_proba1)\n",
    "fpr2, tpr2, thresholds2 = roc_curve(np.array(y_test), y_pred_proba2)\n",
    "fpr3, tpr3, thresholds3 = roc_curve(np.array(y_test), y_pred_proba3)\n",
    "fpr4, tpr4, thresholds4 = roc_curve(np.array(y_test), y_pred_proba4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica ROC\n",
    "green_line = mlines.Line2D([fpr1], [tpr1], color='green', marker='|',\n",
    "                          markersize=17, label='Random Forest')\n",
    "red_line = mlines.Line2D([fpr2], [tpr2], color='red', marker='|',\n",
    "                          markersize=17, label='Logistic Regression')\n",
    "blue_line = mlines.Line2D([fpr3], [tpr3], color='blue', marker='|',\n",
    "                          markersize=17, label='Support Vector Machine')\n",
    "yellow_line = mlines.Line2D([fpr4], [tpr4], color='yellow', marker='|',\n",
    "                          markersize=17, label='Multilayer Perceptron')\n",
    "\n",
    "plt.plot([fpr1], [tpr1], 'g|', [fpr2], [tpr2], 'r|', [fpr3], [tpr3], 'b|', [fpr4], [tpr4], 'y|')\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xlabel('Tasa de Falsos Positivos (1 - Precision)')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos (Recall)')\n",
    "plt.title('Curva ROC para Clasificador de Tsunamis')\n",
    "plt.legend(handles=[green_line, red_line, yellow_line, blue_line], bbox_to_anchor=(0.49, 0.33), loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividiendo el mapa\n",
    "Debido a que todos los algoritmos fallaban al predecir la presencia de tsunami con medidas de performance razonables, trataremos de dividir el mapamundi, para poder acotar el contexto de modelación, y así tratar un conjunto de datos que quizás sea más separable que todos los datos juntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partiremos analizando el continente de america, que según el mapa anteriormente visualizado, estaría en el\n",
    "# siguiente intervalo:\n",
    "\n",
    "# america = quakes_df[(quakes_df['longitude'] < -50) & (quakes_df['longitude'] > -150)]\n",
    "# america = quakes_df[(quakes_df['longitude'] > -50) & (quakes_df['longitude'] < 75)]\n",
    "america = quakes_df[(quakes_df['longitude'] > 150)]\n",
    "x = america['longitude']\n",
    "y = america['latitude']\n",
    "\n",
    "area = 10\n",
    "plt.figure(figsize=(10, 8))\n",
    "max_mag, min_mag = max(america['mag']), min(america['mag'])\n",
    "area = (america['mag'] - min_mag) / (max_mag - min_mag) * 50\n",
    "\n",
    "plt.scatter(x, y, s=area, marker='o',\n",
    "            c=1 - america['tsunami'],\n",
    "            alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "america_features = america[['latitude', 'longitude', 'depth', 'mag']]\n",
    "america_target = america['tsunami']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partición estratificada para america\n",
    "X_train_america, X_test_america, y_train_america, y_test_america = train_test_split(america_features,\n",
    "                                                                                    america_target,\n",
    "                                                                                    test_size=0.3,\n",
    "                                                                                    random_state=1,\n",
    "                                                                                    stratify=america_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST CON SOLO AMERICA\n",
    "display(HTML('<h3> Resultados de métricas evaluadas con Random Forest:\\n</h3>'))\n",
    "\n",
    "trained = fitRandomForest(X_train_america, y_train_america, seed=1)\n",
    "y_pred, y_pred_proba = prediction(X_test, trained)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['no provoca tsunami', 'sí provoca tsunami']))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred), '\\n')   ## accuracy\n",
    "print('Matriz de Confusión:\\n\\n', confusion_matrix(y_test, y_pred), '\\n')   ## Matriz de Confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la precisión en en américa con random forest sube mucho. Sin embargo, el recall es muy bajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESION LOGÍSTICA CON AMERICA\n",
    "display(HTML('<h3> Resultados de métricas evaluadas con Regresion Logística\\n</h3>'))\n",
    "\n",
    "trained = fitRegresionLogistica(X_train_america, y_train_america, 1)\n",
    "y_pred, y_pred_proba = prediction(X_test, trained)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['no provoca tsunami', 'sí provoca tsunami']))\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred), '\\n')   ## accuracy\n",
    "print('Matriz de Confusión:\\n\\n', confusion_matrix(y_test, y_pred), '\\n')   ## Matriz de Confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con regresion logística, el recall es el que llega muy alto, mientras que la precision es muy mala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtiene el set de train\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(quakes_features, target, test_size=0.3, \n",
    "                                                    random_state=1, stratify=target)\n",
    "\n",
    "qk_sampled_X, qk_sampled_y = up_class(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se evalúa el modelo Random Forest con la nueva base de datos de entrenamiento mediante RandomSubsampling \n",
    "\n",
    "listaFscore, listaRecall, listaPrecision, listaAccuracy = RandomSubsampling(qk_sampled_X,\n",
    "                                                                            qk_sampled_y, \n",
    "                                                                            X_test,\n",
    "                                                                            y_test, 20,\n",
    "                                                                            seed=1,\n",
    "                                                                            model=fitRandomForest)\n",
    "\n",
    "f_score = sum(i for i in listaFscore)/len(listaFscore)\n",
    "recall = sum(i for i in listaRecall)/len(listaRecall)\n",
    "precision = sum(i for i in listaPrecision)/len(listaPrecision)\n",
    "accuracy = sum(i for i in listaAccuracy)/len(listaAccuracy)\n",
    "\n",
    "print('Precision: {:.2f}             \\nRecall: {:.2f}\\nF-score: {:.2f}\\nAccuracy: {:.2f}\\n'.format(precision, recall, f_score, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se evalúa el modelo Random Forest con la nueva base de datos de entrenamiento mediante RandomSubsampling \n",
    "\n",
    "listaFscore, listaRecall, listaPrecision, listaAccuracy = RandomSubsampling(qk_sampled_X,\n",
    "                                                                            qk_sampled_y, \n",
    "                                                                            X_test,\n",
    "                                                                            y_test, 20,\n",
    "                                                                            seed=1,\n",
    "                                                                            model=fitRegresionLogistica)\n",
    "\n",
    "f_score = sum(i for i in listaFscore)/len(listaFscore)\n",
    "recall = sum(i for i in listaRecall)/len(listaRecall)\n",
    "precision = sum(i for i in listaPrecision)/len(listaPrecision)\n",
    "accuracy = sum(i for i in listaAccuracy)/len(listaAccuracy)\n",
    "\n",
    "print('Precision: {:.2f}             \\nRecall: {:.2f}\\nF-score: {:.2f}\\nAccuracy: {:.2f}\\n'.format(precision, recall, f_score, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ve claramente en los resultados que la predicción ha mejorado relativamente, al aplicar el modelo de Random Forest con Up-Sample Minority Class. El Recall ha aumentado de un 29,3 % a un ~ 81 %, sin embargo, la Precisión se desplomó desde un 71,7 % a un 25,48 %. En cuanto a la Accuracy y F-Score, estos disminuyeron entre 4 y 5 puntos aproximadamente. Para efectos prácticos, pese a que al ocupar Up Sample Minority Class empeoró la predicción, al ocupar este método se predice significativamente mejor los tsunamis que van a ocurrir. Esto es importante de considerar, ya que predecir un tsunami es muy importante para salvar vidas y lo anterior contribuye a poder determinar con rapidez medidas de contingencia. Desde otro punto de vista, dado que el modelo es poco preciso, también se predicen muchos tsunamis cuando estos no ocurren, lo que puede ser perjudicial desde el punto de vista de costos, ya que en la práctica se utilizarían muchos recursos de todo tipo en vano y habrían pérdidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
